{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds\nimport os\nimport time\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/chess/games.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#indexing out onlly the useful data\ndata = data[[\"moves\", \"winner\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocessing the data\n#white will be 1, black will be 0\ndata.winner = data.winner.apply(lambda x : 1 if x == \"white\" else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenizing the moves list\ntokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(data.moves, target_vocab_size = 2e15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenize the data\ndata.moves = data.moves.apply(lambda x : tokenizer.encode(x))\n\n# #removing rows with too many tokens so the model can train faster\ndata.moves = data.moves.apply(lambda x : x[:100] if len(x) > 100 else x)\n\n#Add start and end tokens\ndata.moves = data.moves.apply(lambda x : [tokenizer.vocab_size] + x + [tokenizer.vocab_size + 1])\n\n# #removing rows with too many tokens so the model can train faster\n# data = data[data.moves.apply(lambda x : len(x) <= 100)].reset_index(drop = True)\n\n#pad the data moves to fit into the neural network\ndata.moves = pd.Series(list(tf.keras.preprocessing.sequence.pad_sequences(data.moves, padding = \"post\")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert to tf dataset to easily fit the NN\ntf_data = tf.data.Dataset.from_tensor_slices((list(data.moves), list(data.winner)))\ntf_data = tf_data.cache()\ntf_data = tf_data.shuffle(10).padded_batch(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#strategy for Neural Network\n#Will build the Encoder portion of the Transformer for the NN to make its own representation of the board, and then will add on top\n    #a few dense layers to output the prediction\n#Will still perform positional encoding, but will not do any type of masking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining necessary functions to create the Encoder\n#Code from https://www.tensorflow.org/tutorials/text/transformer\n\n#positional encoding to give the NN context about the order of words\n#Signature: 2DArray, 2DArray, Num\n#Effects: return an n x m matrix, where n is the number of pos elements, and m is the dimension of each pos element\ndef get_angles(pos, i, model_depth):\n    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(model_depth))\n    return pos * angle_rates\n\n#Signature: Num, Num\n            #total num words, num of layers in model\n#Effects: Returns a modified result of get_angles to reflect positional encoding\ndef positional_encoding(position, model_depth):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          model_depth)\n\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = angle_rads[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\n#Padding mask so the model does not treat the padded tokens as input\n#Signature: listOfNum (listOfTokens)\n#Effects: Produces 1 if the token in the sequence is a padding token, 0 otherwise\ndef create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n\n    # add extra dimensions to add the padding\n    # to the attention logits.\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n\n#scaled_dot_product_attention\ndef scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"Calculate the attention weights.\n    q, k, v must have matching leading dimensions.\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n    The mask has different shapes depending on its type(padding or look ahead) \n    but it must be broadcastable for addition.\n\n    Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable \n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n    Returns:\n    output, attention_weights\n    \"\"\"\n\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  \n\n    # softmax is normalized on the last axis (seq_len_k) so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output\n\n#Multihead Attention\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n        \n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention = scaled_dot_product_attention(\n            q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention, \n                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output\n\n#Point wise feed forward NN\ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n    ])\n\n#Encoder Layer\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n        #dropout layers aren't necessary per se, but they're good for the NN (like residual connections). They randomly set outgoing\n            #edges of hidden nodes to 0 during training phase to reduce the chance of overfitting\n    def call(self, x, training, mask):\n\n        attn_output = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2\n\n#Encoder\n#The input is embedded and then summed with the positional encoding, and then passed onto the N Encoder layers\nclass Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n               maximum_position_encoding, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                                self.d_model)\n\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                           for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n\n        seq_len = tf.shape(x)[1]\n\n        # adding embedding and position encoding.\n        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)\n    \n#The full model\nclass Model(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n               pe_input, rate=0.1):\n        super(Model, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n                               input_vocab_size, pe_input, rate)\n\n        self.final_layer = tf.keras.layers.Dense(1, activation = \"sigmoid\")\n    \n    def call(self, inp, training, enc_padding_mask):\n\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n        final_output = self.final_layer(enc_output)  # (batch_size, tar_seq_len, 1)\n\n        return final_output\n\n\n    \n#Optimizer, Loss, Accuracy metrics\n# loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction='none')\n\n# def loss_function(real, pred):\n#     mask = tf.math.logical_not(tf.math.equal(real, 0))\n#     loss_ = loss_object(real, pred)\n\n#     mask = tf.cast(mask, dtype=loss_.dtype)\n#     loss_ *= mask\n\n#     return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n\noptimizer = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.98, \n                                     epsilon=1e-9)\nloss_function = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.05, reduction = \"none\")\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hyper paramaters \nnum_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\ninput_vocab_size = tokenizer.vocab_size + 2\ndropout_rate = 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(num_layers, d_model, num_heads, dff, input_vocab_size, input_vocab_size, dropout_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_step(inp, tar):\n\n    enc_padding_mask = create_padding_mask(inp)\n#     tar = tf.reshape(tar, (50,1))\n    with tf.GradientTape() as tape:\n        \n        predictions = model(inp, True, enc_padding_mask)\n        \n\n        loss = loss_function(tar, predictions)\n\n\n    gradients = tape.gradient(loss, model.trainable_variables)    \n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n\n    train_loss(loss)\n    train_accuracy(tar, predictions)\n    \n# Creating checkpoint paths\ncheckpoint_path = \"/kaggle/working/\"\n\nckpt = tf.train.Checkpoint(model=model,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print ('Latest checkpoint restored!!')\n    \ndef train_model(tf_dataset, EPOCHS, verbose_interval):\n    for epoch in range(EPOCHS):\n        start = time.time()\n\n        train_loss.reset_states()\n        train_accuracy.reset_states()\n\n        for (batch, (inp, tar)) in enumerate(tf_dataset):\n            \n            train_step(inp, tar)\n\n            if batch % verbose_interval == 0:\n                  print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n                  epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n\n        if (epoch + 1) % 5 == 0:\n            ckpt_save_path = ckpt_manager.save()\n            print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n                                                                 ckpt_save_path))\n\n        print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n                                                    train_loss.result(), \n                                                    train_accuracy.result()))\n\n        print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n        \ndef evaluate(inp_sentence):\n    start_token = [tokenizer.vocab_size]\n    end_token = [tokenizer.vocab_size + 1]\n  \n    inp_sentence = start_token + inp_sentence + end_token\n    encoder_input = tf.expand_dims(inp_sentence, 0)\n  \n    enc_padding_mask = create_padding_mask(encoder_input)\n    print(enc_padding_mask)\n  \n    # predictions.shape == (batch_size, seq_len, vocab_size)\n    predictions = model(encoder_input, \n                                     False,\n                                     enc_padding_mask)\n    \n    # select the last word from the seq_len dimension\n    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n    \n    return predictions\n\n#     predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n    \n#     # return the result if the predicted_id is equal to the end token\n#     if predicted_id == tokenizer_en.vocab_size+1:\n#       return tf.squeeze(output, axis=0), attention_weights\n    \n#     # concatentate the predicted_id to the output which is given to the decoder\n#     # as its input.\n#     output = tf.concat([output, predicted_id], axis=-1)\n\n#   return tf.squeeze(output, axis=0), attention_weights\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(tf_data, 10, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.save_to_file('/kaggle/working/tokenizer_file')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/input/chess-win-predictor-model-and-tokenizer-checkpoint/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ckpt_manager = tf.train.CheckpointManager(ckpt, os.getcwd(), max_to_keep=1)\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print ('Latest checkpoint restored!!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/working/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle_model = {\"model\" : model}\npickle.dump(pickle, open( 'model_file' + \".p\", \"wb\" ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_seq = data.iloc[0].moves\nevaluate(test_seq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}